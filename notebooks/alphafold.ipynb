{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from foldeval.pyfoldeval.generic import PredictionAlgorithm\n",
        "from IPython.utils import io\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import tqdm.notebook\n",
        "import enum\n",
        "# from alphafold.notebooks import notebook_utils\n",
        "# import collections\n",
        "# import copy\n",
        "# from concurrent import futures\n",
        "# from urllib import request\n",
        "# import json\n",
        "# import random\n",
        "# from alphafold.data import feature_processing\n",
        "# from alphafold.data import msa_pairing\n",
        "# from alphafold.data import pipeline\n",
        "# from alphafold.data import pipeline_multimer\n",
        "# from alphafold.data.tools import jackhmmer\n",
        "# from alphafold.common import protein\n",
        "# from alphafold.model import model\n",
        "# from alphafold.model import config\n",
        "# from alphafold.model import data\n",
        "# from alphafold.relax import relax\n",
        "# from alphafold.relax import utils\n",
        "# from matplotlib import gridspec\n",
        "# from IPython import display\n",
        "# from ipywidgets import GridspecLayout\n",
        "# from ipywidgets import Output\n",
        "# import py3Dmol\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AlphaFold(PredictionAlgorithm):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.msa_results = None\n",
        "    self.plddt_bands = [\n",
        "      (0, 50, '#FF7D45'),\n",
        "      (50, 70, '#FFDB13'),\n",
        "      (70, 90, '#65CBF3'),\n",
        "      (90, 100, '#0053D6')\n",
        "    ]\n",
        "    self.to_visualize_pdb = None\n",
        "    self.pae_outputs = None\n",
        "    self.plddts = None\n",
        "    self.unrelaxed_proteins = None\n",
        "    self.best_model_name = None \n",
        "  \n",
        "\n",
        "  \n",
        "  def setup(self) -> None:\n",
        "    if os.path.isdir('alphafold'): return\n",
        "    with tqdm.notebook.tqdm(total=200, bar_format=self.tqdm_format) as pbar:\n",
        "      self._install_dependencies(pbar)\n",
        "      self._download_repository(pbar)\n",
        "      self._environment_setup()\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "  def do_msa(self) -> None:\n",
        "    global sequences\n",
        "    model_type_to_use = notebook_utils.ModelType.MONOMER\n",
        "\n",
        "    ### deepmind/alphafold/notebooks/AlphaFold.ipynb ===========================\n",
        "    #\n",
        "    # This is not an officially-supported Google product.\n",
        "    #\n",
        "    # This Colab notebook and other information provided is for theoretical \n",
        "    # modelling only, caution should be exercised in its use. It is provided \n",
        "    # ‘as-is’ without any warranty of any kind, whether expressed or implied. \n",
        "    # Information is not intended to be a substitute for professional medical \n",
        "    # advice, diagnosis, or treatment, and does not constitute medical or other \n",
        "    # professional advice.\n",
        "    #\n",
        "    # Copyright 2021 DeepMind Technologies Limited.\n",
        "    #\n",
        "\n",
        "    # --- Find the closest source ---\n",
        "    test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2021_03.fasta.1'\n",
        "    ex = futures.ThreadPoolExecutor(3)\n",
        "    def _af_fetch(source):\n",
        "      request.urlretrieve(test_url_pattern.format(source))\n",
        "      return source\n",
        "    fs = [ex.submit(_af_fetch, source) for source in ['', '-europe', '-asia']]\n",
        "    source = None\n",
        "    for f in futures.as_completed(fs):\n",
        "      source = f.result()\n",
        "      ex.shutdown()\n",
        "      break\n",
        "    JACKHMMER_BINARY_PATH = '/usr/bin/jackhmmer'\n",
        "    DB_ROOT_PATH = f'https://storage.googleapis.com/alphafold-colab{source}/latest/'\n",
        "    # The z_value is the number of sequences in a database.\n",
        "    MSA_DATABASES = [\n",
        "      {'db_name': 'uniref90',\n",
        "      'db_path': f'{DB_ROOT_PATH}uniref90_2021_03.fasta',\n",
        "      'num_streamed_chunks': 59,\n",
        "      'z_value': 135_301_051},\n",
        "      {'db_name': 'smallbfd',\n",
        "      'db_path': f'{DB_ROOT_PATH}bfd-first_non_consensus_sequences.fasta',\n",
        "      'num_streamed_chunks': 17,\n",
        "      'z_value': 65_984_053},\n",
        "      {'db_name': 'mgnify',\n",
        "      'db_path': f'{DB_ROOT_PATH}mgy_clusters_2019_05.fasta',\n",
        "      'num_streamed_chunks': 71,\n",
        "      'z_value': 304_820_129},\n",
        "    ]\n",
        "    # Search UniProt and construct the all_seq features only for heteromers, not homomers.\n",
        "    if model_type_to_use == notebook_utils.ModelType.MULTIMER and len(set(sequences)) > 1:\n",
        "      MSA_DATABASES.extend([\n",
        "          # Swiss-Prot and TrEMBL are concatenated together as UniProt.\n",
        "          {'db_name': 'uniprot',\n",
        "          'db_path': f'{DB_ROOT_PATH}uniprot_2021_03.fasta',\n",
        "          'num_streamed_chunks': 98,\n",
        "          'z_value': 219_174_961 + 565_254},\n",
        "      ])\n",
        "    TOTAL_JACKHMMER_CHUNKS = sum(\n",
        "      [cfg['num_streamed_chunks'] for cfg in MSA_DATABASES])\n",
        "    MAX_HITS = {\n",
        "      'uniref90': 10_000,\n",
        "      'smallbfd': 5_000,\n",
        "      'mgnify': 501,\n",
        "      'uniprot': 50_000,\n",
        "    }\n",
        "    TQDM_BAR_FORMAT = self.tqdm_format\n",
        "    def _af_msa_get_msa(fasta_path):\n",
        "      \"\"\"Searches for MSA for the given sequence using chunked Jackhmmer search.\"\"\"\n",
        "      # Run the search against chunks of genetic databases (since the genetic\n",
        "      # databases don't fit in Colab disk).\n",
        "      raw_msa_results = collections.defaultdict(list)\n",
        "      with tqdm.notebook.tqdm(total=TOTAL_JACKHMMER_CHUNKS, bar_format=TQDM_BAR_FORMAT) as pbar:\n",
        "        def _af_jackhmmer_chunk_callback(i): pbar.update(n=1)\n",
        "        for db_config in MSA_DATABASES:\n",
        "          db_name = db_config['db_name']\n",
        "          pbar.set_description(f'Searching {db_name}')\n",
        "          jackhmmer_runner = jackhmmer.Jackhmmer(\n",
        "              binary_path=JACKHMMER_BINARY_PATH,\n",
        "              database_path=db_config['db_path'],\n",
        "              get_tblout=True,\n",
        "              num_streamed_chunks=db_config['num_streamed_chunks'],\n",
        "              streaming_callback=_af_jackhmmer_chunk_callback,\n",
        "              z_value=db_config['z_value'])\n",
        "          # Group the results by database name.\n",
        "          raw_msa_results[db_name].extend(jackhmmer_runner.query(fasta_path))\n",
        "      return raw_msa_results\n",
        "    features_for_chain = {}\n",
        "    raw_msa_results_for_sequence = {}\n",
        "    for sequence_index, sequence in enumerate(sequences, start=1):\n",
        "      print(f'\\nGetting MSA for sequence {sequence_index}')\n",
        "      fasta_path = f'target_{sequence_index}.fasta'\n",
        "      with open(fasta_path, 'wt') as f:\n",
        "        f.write(f'>query\\n{sequence}')\n",
        "      # Don't do redundant work for multiple copies of the same chain in the multimer.\n",
        "      if sequence not in raw_msa_results_for_sequence:\n",
        "        raw_msa_results = _af_msa_get_msa(fasta_path=fasta_path)\n",
        "        raw_msa_results_for_sequence[sequence] = raw_msa_results\n",
        "      else:\n",
        "        raw_msa_results = copy.deepcopy(raw_msa_results_for_sequence[sequence])\n",
        "      # Extract the MSAs from the Stockholm files.\n",
        "      # NB: deduplication happens later in pipeline.make_msa_features.\n",
        "      single_chain_msas = []\n",
        "      uniprot_msa = None\n",
        "      for db_name, db_results in raw_msa_results.items():\n",
        "        merged_msa = notebook_utils.merge_chunked_msa(\n",
        "            results=db_results, max_hits=MAX_HITS.get(db_name))\n",
        "        if merged_msa.sequences and db_name != 'uniprot':\n",
        "          single_chain_msas.append(merged_msa)\n",
        "          msa_size = len(set(merged_msa.sequences))\n",
        "          print(f'{msa_size} unique sequences found in {db_name} for sequence {sequence_index}')\n",
        "        elif merged_msa.sequences and db_name == 'uniprot':\n",
        "          uniprot_msa = merged_msa\n",
        "      notebook_utils.show_msa_info(single_chain_msas=single_chain_msas, sequence_index=sequence_index)\n",
        "      # Turn the raw data into model features.\n",
        "      feature_dict = {}\n",
        "      feature_dict.update(pipeline.make_sequence_features(\n",
        "          sequence=sequence, description='query', num_res=len(sequence)))\n",
        "      feature_dict.update(pipeline.make_msa_features(msas=single_chain_msas))\n",
        "      # We don't use templates in AlphaFold Colab notebook, add only empty placeholder features.\n",
        "      feature_dict.update(notebook_utils.empty_placeholder_template_features(\n",
        "          num_templates=0, num_res=len(sequence)))\n",
        "      # Construct the all_seq features only for heteromers, not homomers.\n",
        "      if model_type_to_use == notebook_utils.ModelType.MULTIMER and len(set(sequences)) > 1:\n",
        "        valid_feats = msa_pairing.MSA_FEATURES + ('msa_species_identifiers',)\n",
        "        all_seq_features = {\n",
        "            f'{k}_all_seq': v for k, v in pipeline.make_msa_features([uniprot_msa]).items()\n",
        "            if k in valid_feats}\n",
        "        feature_dict.update(all_seq_features)\n",
        "      features_for_chain[protein.PDB_CHAIN_IDS[sequence_index - 1]] = feature_dict\n",
        "    # Do further feature post-processing depending on the model type.\n",
        "    if model_type_to_use == notebook_utils.ModelType.MONOMER:\n",
        "      self.msa_results = features_for_chain[protein.PDB_CHAIN_IDS[0]]\n",
        "    elif model_type_to_use == notebook_utils.ModelType.MULTIMER:\n",
        "      all_chain_features = {}\n",
        "      for chain_id, chain_features in features_for_chain.items():\n",
        "        all_chain_features[chain_id] = pipeline_multimer.convert_monomer_features(\n",
        "            chain_features, chain_id)\n",
        "      all_chain_features = pipeline_multimer.add_assembly_features(all_chain_features)\n",
        "      np_example = feature_processing.pair_and_merge(\n",
        "          all_chain_features=all_chain_features)\n",
        "      # Pad MSA to avoid zero-sized extra_msa.\n",
        "      self.msa_results = pipeline_multimer.pad_msa(np_example, min_num_seq=512)\n",
        "      # deepmind/alphafold/notebooks/AlphaFold.ipynb ===========================\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "  def make_prediction(self) -> str: \n",
        "    global jobname\n",
        "    global af_run_relax\n",
        "    global af_relax_use_gpu\n",
        "    model_type_to_use = notebook_utils.ModelType.MONOMER\n",
        "    np_example = self.msa_results\n",
        "    \n",
        "    ### deepmind/alphafold/notebooks/AlphaFold.ipynb ===========================\n",
        "    #\n",
        "    # This is not an officially-supported Google product.\n",
        "    #\n",
        "    # This Colab notebook and other information provided is for theoretical \n",
        "    # modelling only, caution should be exercised in its use. It is provided \n",
        "    # ‘as-is’ without any warranty of any kind, whether expressed or implied. \n",
        "    # Information is not intended to be a substitute for professional medical \n",
        "    # advice, diagnosis, or treatment, and does not constitute medical or other \n",
        "    # professional advice.\n",
        "    #\n",
        "    # Copyright 2021 DeepMind Technologies Limited.\n",
        "    #\n",
        "\n",
        "    # --- Run the model ---\n",
        "    if model_type_to_use == notebook_utils.ModelType.MONOMER:\n",
        "      model_names = config.MODEL_PRESETS['monomer'] + ('model_2_ptm',)\n",
        "    elif model_type_to_use == notebook_utils.ModelType.MULTIMER:\n",
        "      model_names = config.MODEL_PRESETS['multimer']\n",
        "    output_dir = jobname\n",
        "    plddts = {}\n",
        "    ranking_confidences = {}\n",
        "    pae_outputs = {}\n",
        "    unrelaxed_proteins = {}\n",
        "    with tqdm.notebook.tqdm(total=len(model_names) + 1, bar_format=self.tqdm_format) as pbar:\n",
        "      for model_name in model_names:\n",
        "        pbar.set_description(f'Running {model_name}')\n",
        "        cfg = config.model_config(model_name)\n",
        "        if model_type_to_use == notebook_utils.ModelType.MONOMER:\n",
        "          cfg.data.eval.num_ensemble = 1\n",
        "        elif model_type_to_use == notebook_utils.ModelType.MULTIMER:\n",
        "          cfg.model.num_ensemble_eval = 1\n",
        "        params = data.get_model_haiku_params(model_name, './alphafold/data')\n",
        "        model_runner = model.RunModel(cfg, params)\n",
        "        processed_feature_dict = model_runner.process_features(np_example, random_seed=0)\n",
        "        prediction = model_runner.predict(processed_feature_dict, random_seed=random.randrange(sys.maxsize))\n",
        "        mean_plddt = prediction['plddt'].mean()\n",
        "        if model_type_to_use == notebook_utils.ModelType.MONOMER:\n",
        "          if 'predicted_aligned_error' in prediction:\n",
        "            pae_outputs[model_name] = (prediction['predicted_aligned_error'],\n",
        "                                      prediction['max_predicted_aligned_error'])\n",
        "          else:\n",
        "            # Monomer models are sorted by mean pLDDT. Do not put monomer pTM models here as they\n",
        "            # should never get selected.\n",
        "            ranking_confidences[model_name] = prediction['ranking_confidence']\n",
        "            plddts[model_name] = prediction['plddt']\n",
        "        elif model_type_to_use == notebook_utils.ModelType.MULTIMER:\n",
        "          # Multimer models are sorted by pTM+ipTM.\n",
        "          ranking_confidences[model_name] = prediction['ranking_confidence']\n",
        "          plddts[model_name] = prediction['plddt']\n",
        "          pae_outputs[model_name] = (prediction['predicted_aligned_error'],\n",
        "                                    prediction['max_predicted_aligned_error'])\n",
        "        # Set the b-factors to the per-residue plddt.\n",
        "        final_atom_mask = prediction['structure_module']['final_atom_mask']\n",
        "        b_factors = prediction['plddt'][:, None] * final_atom_mask\n",
        "        unrelaxed_protein = protein.from_prediction(\n",
        "            processed_feature_dict,\n",
        "            prediction,\n",
        "            b_factors=b_factors,\n",
        "            remove_leading_feature_dimension=(\n",
        "                model_type_to_use == notebook_utils.ModelType.MONOMER))\n",
        "        unrelaxed_proteins[model_name] = unrelaxed_protein\n",
        "        # Delete unused outputs to save memory.\n",
        "        del model_runner\n",
        "        del params\n",
        "        del prediction\n",
        "        pbar.update(n=1)\n",
        "      # --- AMBER relax the best model ---\n",
        "      # Find the best model according to the mean pLDDT.\n",
        "      best_model_name = max(ranking_confidences.keys(), key=lambda x: ranking_confidences[x])\n",
        "      if af_run_relax:\n",
        "        pbar.set_description(f'AMBER relaxation')\n",
        "        amber_relaxer = relax.AmberRelaxation(\n",
        "            max_iterations=0,\n",
        "            tolerance=2.39,\n",
        "            stiffness=10.0,\n",
        "            exclude_residues=[],\n",
        "            max_outer_iterations=3,\n",
        "            use_gpu=af_relax_use_gpu)\n",
        "        relaxed_pdb, _, _ = amber_relaxer.process(prot=unrelaxed_proteins[best_model_name])\n",
        "      else:\n",
        "        print('Warning: Running without the relaxation stage.')\n",
        "        relaxed_pdb = protein.to_pdb(unrelaxed_proteins[best_model_name])\n",
        "      pbar.update(n=1)  # Finished AMBER relax.\n",
        "    # Construct multiclass b-factors to indicate confidence bands\n",
        "    # 0=very low, 1=low, 2=confident, 3=very high\n",
        "    banded_b_factors = []\n",
        "    for plddt in plddts[best_model_name]:\n",
        "      for idx, (min_val, max_val, _) in enumerate(self.plddt_bands):\n",
        "        if plddt >= min_val and plddt <= max_val:\n",
        "          banded_b_factors.append(idx)\n",
        "          break\n",
        "    banded_b_factors = np.array(banded_b_factors)[:, None] * final_atom_mask\n",
        "    to_visualize_pdb = utils.overwrite_b_factors(relaxed_pdb, banded_b_factors)\n",
        "    # Write out the prediction\n",
        "    pred_output_path = os.path.join(output_dir, 'af_prediction.pdb')\n",
        "    with open(pred_output_path, 'w') as f: f.write(relaxed_pdb)\n",
        "    # deepmind/alphafold/notebooks/AlphaFold.ipynb =============================\n",
        "    \n",
        "    self.plddt_scores = 0.01 * plddts[best_model_name]\n",
        "    self.pred_output_path = pred_output_path\n",
        "    self.to_visualize_pdb = to_visualize_pdb\n",
        "    self.pae_outputs = pae_outputs\n",
        "    self.plddts = plddts\n",
        "    self.unrelaxed_proteins = unrelaxed_proteins\n",
        "    self.best_model_name = best_model_name \n",
        "\n",
        "    # se modificó el código original para eleminiar las moléculas OXT que \n",
        "    # estaban provocando errores con el programa lddt de OpenStructure\n",
        "    corrected_output_filename = f'{jobname}/af_prediction_without_oxt_atoms.pdb'\n",
        "    with open(corrected_output_filename, 'w') as file:\n",
        "      for line in open(pred_output_path, 'r'):\n",
        "        if line[:4] == 'ATOM':\n",
        "          if line.split()[2] != 'OXT': file.write(line)\n",
        "    return corrected_output_filename\n",
        "\n",
        "\n",
        "\n",
        "  def display_predicted_model(self) -> None: \n",
        "    global jobname\n",
        "    global dpi\n",
        "    global show_sidechains\n",
        "    model_type_to_use = notebook_utils.ModelType.MONOMER\n",
        "    output_dir = jobname\n",
        "\n",
        "    ### deepmind/alphafold/notebooks/AlphaFold.ipynb ===========================\n",
        "    #\n",
        "    # This is not an officially-supported Google product.\n",
        "    #\n",
        "    # This Colab notebook and other information provided is for theoretical \n",
        "    # modelling only, caution should be exercised in its use. It is provided \n",
        "    # ‘as-is’ without any warranty of any kind, whether expressed or implied. \n",
        "    # Information is not intended to be a substitute for professional medical \n",
        "    # advice, diagnosis, or treatment, and does not constitute medical or other \n",
        "    # professional advice.\n",
        "    #\n",
        "    # Copyright 2021 DeepMind Technologies Limited.\n",
        "    #\n",
        "\n",
        "    # --- Visualise the prediction & confidence ---\n",
        "    def _af_plot_plddt_legend():\n",
        "      \"\"\"Plots the legend for pLDDT.\"\"\"\n",
        "      thresh = ['Very low (pLDDT < 50)',\n",
        "                'Low (70 > pLDDT > 50)',\n",
        "                'Confident (90 > pLDDT > 70)',\n",
        "                'Very high (pLDDT > 90)']\n",
        "\n",
        "      colors = [x[2] for x in self.plddt_bands]\n",
        "      plt.figure(figsize=(2, 2))\n",
        "      for c in colors:\n",
        "        plt.bar(0, 0, color=c)\n",
        "      plt.legend(thresh, frameon=False, loc='center', fontsize=20)\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      ax = plt.gca()\n",
        "      ax.spines['right'].set_visible(False)\n",
        "      ax.spines['top'].set_visible(False)\n",
        "      ax.spines['left'].set_visible(False)\n",
        "      ax.spines['bottom'].set_visible(False)\n",
        "      plt.title('Model Confidence', fontsize=20, pad=20)\n",
        "      return plt\n",
        "    # Show the structure coloured by chain if the multimer model has been used.\n",
        "    if model_type_to_use == notebook_utils.ModelType.MULTIMER:\n",
        "      multichain_view = py3Dmol.view(width=800, height=600)\n",
        "      multichain_view.addModelsAsFrames(self.to_visualize_pdb)\n",
        "      multichain_style = {'cartoon': {'colorscheme': 'chain'}}\n",
        "      multichain_view.setStyle({'model': -1}, multichain_style)\n",
        "      multichain_view.zoomTo()\n",
        "      multichain_view.show()\n",
        "    # Color the structure by per-residue pLDDT\n",
        "    color_map = {i: bands[2] for i, bands in enumerate(self.plddt_bands)}\n",
        "    view = py3Dmol.view(width=800, height=600)\n",
        "    view.addModelsAsFrames(self.to_visualize_pdb)\n",
        "    style = {'cartoon': {'colorscheme': {'prop': 'b', 'map': color_map}}}\n",
        "    if show_sidechains:\n",
        "      style['stick'] = {}\n",
        "    view.setStyle({'model': -1}, style)\n",
        "    view.zoomTo()\n",
        "    grid = GridspecLayout(1, 2)\n",
        "    out = Output()\n",
        "    with out:\n",
        "      view.show()\n",
        "    grid[0, 0] = out\n",
        "    out = Output()\n",
        "    with out:\n",
        "      _af_plot_plddt_legend().show()\n",
        "    grid[0, 1] = out\n",
        "    display.display(grid)\n",
        "    # Display pLDDT and predicted aligned error (if output by the model).\n",
        "    if self.pae_outputs:\n",
        "      num_plots = 2\n",
        "    else:\n",
        "      num_plots = 1\n",
        "    plt.figure(figsize=[8 * num_plots, 6], dpi=dpi)\n",
        "    plt.subplot(1, num_plots, 1)\n",
        "    plt.plot(self.plddt_scores)\n",
        "    plt.title('Predicted LDDT')\n",
        "    plt.xlabel('Residue')\n",
        "    plt.ylabel('pLDDT')\n",
        "    if num_plots == 2:\n",
        "      plt.subplot(1, 2, 2)\n",
        "      pae, max_pae = list(self.pae_outputs.values())[0]\n",
        "      plt.imshow(pae, vmin=0., vmax=max_pae, cmap='Greens_r')\n",
        "      plt.colorbar(fraction=0.046, pad=0.04)\n",
        "      # Display lines at chain boundaries.\n",
        "      best_unrelaxed_prot = self.unrelaxed_proteins[self.best_model_name]\n",
        "      total_num_res = best_unrelaxed_prot.residue_index.shape[-1]\n",
        "      chain_ids = best_unrelaxed_prot.chain_index\n",
        "      for chain_boundary in np.nonzero(chain_ids[:-1] - chain_ids[1:]):\n",
        "        if chain_boundary.size:\n",
        "          plt.plot([0, total_num_res], [chain_boundary, chain_boundary], color='red')\n",
        "          plt.plot([chain_boundary, chain_boundary], [0, total_num_res], color='red')\n",
        "      plt.title('Predicted Aligned Error')\n",
        "      plt.xlabel('Scored residue')\n",
        "      plt.ylabel('Aligned residue')\n",
        "    plt.savefig(f\"{jobname}/plddt-pae.png\", bbox_inches='tight', dpi=dpi)\n",
        "    # Save the predicted aligned error (if it exists).\n",
        "    pae_output_path = os.path.join(output_dir, 'predicted_aligned_error.json')\n",
        "    if self.pae_outputs:\n",
        "      # Save predicted aligned error in the same format as the AF EMBL DB.\n",
        "      pae_data = notebook_utils.get_pae_json(pae=pae, max_pae=max_pae.item())\n",
        "      with open(pae_output_path, 'w') as f:\n",
        "        f.write(pae_data)\n",
        "    # deepmind/alphafold/notebooks/AlphaFold.ipynb =============================\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "  def prepare_results_folder(self) -> str: \n",
        "    return super().prepare_results_folder()\n",
        "\n",
        "\n",
        "\n",
        "  def _install_dependencies(self, pbar) -> None:\n",
        "    ### deepmind/alphafold/notebooks/AlphaFold.ipynb ===========================\n",
        "    #\n",
        "    # This is not an officially-supported Google product.\n",
        "    #\n",
        "    # This Colab notebook and other information provided is for theoretical \n",
        "    # modelling only, caution should be exercised in its use. It is provided \n",
        "    # ‘as-is’ without any warranty of any kind, whether expressed or implied. \n",
        "    # Information is not intended to be a substitute for professional medical \n",
        "    # advice, diagnosis, or treatment, and does not constitute medical or other \n",
        "    # professional advice.\n",
        "    #\n",
        "    # Copyright 2021 DeepMind Technologies Limited.\n",
        "    #\n",
        "    try:\n",
        "      with io.capture_output() as captured:\n",
        "        # Uninstall default Colab version of TF.\n",
        "        %shell pip uninstall -y tensorflow\n",
        "\n",
        "        # [HMMER](http://hmmer.org/) es una librería para buscar homólogos \n",
        "        # en una base de datos de secuencias, y para hacer alineación de \n",
        "        # secuencias usando cadenas ocultas de Markov.\n",
        "        %shell sudo apt install --quiet --yes hmmer\n",
        "        pbar.update(6)\n",
        "\n",
        "        # [Py3DMol](https://github.com/avirshup/py3dmol) es una librería para\n",
        "        # visualizar moléculas tridimensionales en un iPython Notebook \n",
        "        # (_aparentemente está fuera de mantenimiento_).\n",
        "        %shell pip install py3dmol\n",
        "        pbar.update(2)\n",
        "\n",
        "        # Install OpenMM and pdbfixer.\n",
        "        # [OpenMM](https://openmm.org/) es un toolkit para hacer simulación \n",
        "        # molecular.\n",
        "        # [PDBFixer](https://github.com/openmm/pdbfixer) \n",
        "        # es un sanitizador de archivos PDB.\n",
        "        %shell rm -rf /opt/conda\n",
        "        %shell wget -q -P /tmp \\\n",
        "          https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n",
        "            && bash /tmp/Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n",
        "            && rm /tmp/Miniconda3-latest-Linux-x86_64.sh\n",
        "        pbar.update(9)\n",
        "\n",
        "        PATH=%env PATH\n",
        "        %env PATH=/opt/conda/bin:{PATH}\n",
        "        %shell conda install -qy conda==4.13.0 \\\n",
        "            && conda install -qy -c conda-forge \\\n",
        "              python=3.8 \\\n",
        "              openmm=7.5.1 \\\n",
        "              pdbfixer\n",
        "        pbar.update(80)\n",
        "\n",
        "        # Create a ramdisk to store a database chunk to make Jackhmmer run fast.\n",
        "        # (ver [RAM drive](https://en.wikipedia.org/wiki/RAM_drive)).\n",
        "        # [Jackhmmer](https://link.springer.com/article/10.1186/1471-2105-11-431)\n",
        "        # es un algoritmo de búsqueda iterativa de secuencias usando cadenas \n",
        "        # ocultas de Markov que además forma parte de OpenMM.\n",
        "        %shell sudo mkdir -m 777 --parents /tmp/ramdisk\n",
        "        %shell sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
        "        pbar.update(2)\n",
        "\n",
        "        %shell wget -q -P /content \\\n",
        "          https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "        pbar.update(1)\n",
        "    except subprocess.CalledProcessError:\n",
        "      print(captured)\n",
        "      raise\n",
        "    # deepmind/alphafold/notebooks/AlphaFold.ipynb =============================\n",
        "    return\n",
        "  \n",
        "\n",
        "\n",
        "  def _download_repository(self, pbar) -> None:\n",
        "    ### deepmind/alphafold/notebooks/AlphaFold.ipynb ===========================\n",
        "    #\n",
        "    # This is not an officially-supported Google product.\n",
        "    #\n",
        "    # This Colab notebook and other information provided is for theoretical \n",
        "    # modelling only, caution should be exercised in its use. It is provided \n",
        "    # ‘as-is’ without any warranty of any kind, whether expressed or implied. \n",
        "    # Information is not intended to be a substitute for professional medical \n",
        "    # advice, diagnosis, or treatment, and does not constitute medical or other \n",
        "    # professional advice.\n",
        "    #\n",
        "    # Copyright 2021 DeepMind Technologies Limited.\n",
        "    #\n",
        "    GIT_REPO = 'https://github.com/deepmind/alphafold'\n",
        "    SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_colab_2022-03-02.tar'\n",
        "    PARAMS_DIR = './alphafold/data/params'\n",
        "    PARAMS_PATH = os.path.join(PARAMS_DIR, os.path.basename(SOURCE_URL))\n",
        "    try:\n",
        "      with io.capture_output() as captured:\n",
        "        %shell rm -rf alphafold\n",
        "        %shell git clone --branch main {GIT_REPO} alphafold\n",
        "        pbar.update(8)\n",
        "        # Install the required versions of all dependencies.\n",
        "        %shell pip3 install -r ./alphafold/requirements.txt\n",
        "        # Run setup.py to install only AlphaFold.\n",
        "        %shell pip3 install --no-dependencies ./alphafold\n",
        "        pbar.update(10)\n",
        "        # Consulta [esta página](https://stackoverflow.com/questions/43658870/requirements-txt-vs-setup-py)\n",
        "        # para conocer la diferencia (teórica) entre ambos archivos.\n",
        "        # Se desconoce si realmente es necesario ejecutar ambos archivos a\n",
        "        # pesar de que instalan las mismas dependencias.\n",
        "\n",
        "        # Apply OpenMM patch.\n",
        "        # Se desconoce el origen y propósito de este parche\n",
        "        %shell pushd /opt/conda/lib/python3.8/site-packages/ && \\\n",
        "            patch -p0 < /content/alphafold/docker/openmm.patch && \\\n",
        "            popd\n",
        "\n",
        "        # Make sure stereo_chemical_props.txt is in all locations where it could be searched for.\n",
        "        %shell mkdir -p /content/alphafold/alphafold/common\n",
        "        %shell cp -f /content/stereo_chemical_props.txt /content/alphafold/alphafold/common\n",
        "        %shell mkdir -p /opt/conda/lib/python3.8/site-packages/alphafold/common/\n",
        "        %shell cp -f /content/stereo_chemical_props.txt /opt/conda/lib/python3.8/site-packages/alphafold/common/\n",
        "\n",
        "        # Consulta \n",
        "        # [esta página](https://github.com/deepmind/alphafold#model-parameters)\n",
        "        # para conocer más detalles. La carpeta descargada pesa 3.8GB.\n",
        "        %shell mkdir --parents \"{PARAMS_DIR}\"\n",
        "        %shell wget -O \"{PARAMS_PATH}\" \"{SOURCE_URL}\"\n",
        "        pbar.update(27)\n",
        "\n",
        "        %shell tar --extract --verbose --file=\"{PARAMS_PATH}\" \\\n",
        "          --directory=\"{PARAMS_DIR}\" --preserve-permissions\n",
        "        %shell rm \"{PARAMS_PATH}\"\n",
        "        pbar.update(55)\n",
        "    except subprocess.CalledProcessError:\n",
        "      print(captured)\n",
        "      raise\n",
        "    # deepmind/alphafold/notebooks/AlphaFold.ipynb =============================\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "  def _environment_setup(self) -> None:\n",
        "    ### deepmind/alphafold/notebooks/AlphaFold.ipynb ===========================\n",
        "    #\n",
        "    # This is not an officially-supported Google product.\n",
        "    #\n",
        "    # This Colab notebook and other information provided is for theoretical \n",
        "    # modelling only, caution should be exercised in its use. It is provided \n",
        "    # ‘as-is’ without any warranty of any kind, whether expressed or implied. \n",
        "    # Information is not intended to be a substitute for professional medical \n",
        "    # advice, diagnosis, or treatment, and does not constitute medical or other \n",
        "    # professional advice.\n",
        "    #\n",
        "    # Copyright 2021 DeepMind Technologies Limited.\n",
        "    #\n",
        "    import jax\n",
        "    if jax.local_devices()[0].platform == 'tpu':\n",
        "      raise RuntimeError('Colab TPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "    elif jax.local_devices()[0].platform == 'cpu':\n",
        "      raise RuntimeError('Colab CPU runtime not supported. Change it to GPU via Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "    else:\n",
        "      print(f'Running JAX with {jax.local_devices()[0].device_kind} GPU')\n",
        "    \n",
        "    sys.path.append('/opt/conda/lib/python3.8/site-packages')\n",
        "    sys.path.append('/content/alphafold')\n",
        "\n",
        "    # Consultar \n",
        "    # [esta página](https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html)\n",
        "    # para conocer los detalles sobre la configuración de JAX.\n",
        "    os.environ['TF_FORCE_UNIFIED_MEMORY'] = '1'\n",
        "    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '2.0'\n",
        "    # deepmind/alphafold/notebooks/AlphaFold.ipynb =============================\n",
        "    return"
      ],
      "metadata": {
        "id": "VA3cEISFZBv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}